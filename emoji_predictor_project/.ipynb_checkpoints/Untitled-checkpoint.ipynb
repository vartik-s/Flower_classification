{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dictionary for 5 emojis used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary={\n",
    "    \"0\":\":beating_heart:\",\n",
    "    \"1\":\":baseball:\",\n",
    "    \"2\":\":beaming_face_with_smiling_eyes:\",\n",
    "    \"3\":\":downcast_face_with_sweat:\",\n",
    "    \"4\":\":fork_and_knife:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"dataset/train_emoji.csv\")\n",
    "test=pd.read_csv(\"dataset/test_emoji.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>never talk to me again</th>\n",
       "      <th>3</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love you mum</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop saying bullshit</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>congratulations on your acceptance</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The assignment is too long</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I want to go play</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>she did not answer my text</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               never talk to me again  3  Unnamed: 2 Unnamed: 3\n",
       "0     I am proud of your achievements  2         NaN        NaN\n",
       "1      It is the worst day in my life  3         NaN        NaN\n",
       "2                    Miss you so much  0         NaN        [0]\n",
       "3                        food is life  4         NaN        NaN\n",
       "4                      I love you mum  0         NaN        NaN\n",
       "5                Stop saying bullshit  3         NaN        NaN\n",
       "6  congratulations on your acceptance  2         NaN        NaN\n",
       "7         The assignment is too long   3         NaN        NaN\n",
       "8                   I want to go play  1         NaN        [3]\n",
       "9         she did not answer my text   3         NaN        NaN"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only first two columns contain useful information \n",
    "x_train=train[:,0]\n",
    "y_train=train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131,) (131,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55,) (55,)\n"
     ]
    }
   ],
   "source": [
    "test=test.values\n",
    "x_test=test[:,0]\n",
    "y_test=test[:,1]\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am proud of your achievements üòÅ\n",
      "It is the worst day in my life üòì\n",
      "Miss you so much üíì\n",
      "food is life üç¥\n",
      "I love you mum üíì\n"
     ]
    }
   ],
   "source": [
    "#test the created data \n",
    "for i in range(5):\n",
    "    print(x_train[i],emoji.emojize(emoji_dictionary.get(str(y_train[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y into categorical data\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 5) (55, 5)\n"
     ]
    }
   ],
   "source": [
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from the text file \"glove.6B.50d.txt\"\n",
    "f=open(\"glove.6B.50d.txt\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "for line in f:\n",
    "    data=line.split()\n",
    "    word=data[0]\n",
    "    values=np.array(data[1:],dtype=\"float32\")\n",
    "    embeddings[word]=values\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n"
     ]
    }
   ],
   "source": [
    "#test the created dictionary against some random word\n",
    "print(embeddings[\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[\"shape\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting sentences into vectors using embeddings dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are using a pretrained embedding, therefore no need to create an embedding layer in our model.\n",
    "* The output of the embedding layer is a 3D volume.\n",
    "* The output volume is of the shape (batch_size,max_length_of_sentence,size_of_embedding).\n",
    "* In this model we limit the max_length_of_sentence to some small number say 10 and the size of embedding is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_output(X,maxlen=10):\n",
    "    embedding_output=np.zeros((X.shape[0],maxlen,50))\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i]=X[i].split()\n",
    "        for j in range(len(X[i])):\n",
    "            try:\n",
    "                embedding_output[i][j]=embeddings[X[i][j].lower()]\n",
    "            except:\n",
    "                embedding_output[i][j]=np.zeros((50,))\n",
    "    return embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_x_train=embedding_output(x_train)\n",
    "embedded_x_test=embedding_output(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 10, 50) (55, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedded_x_train.shape,embedded_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 141,381\n",
      "Trainable params: 141,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(10,50),return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "Checkpoint=ModelCheckpoint(\"best_model.h5\",monitor='val_acc',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104 samples, validate on 27 samples\n",
      "Epoch 1/80\n",
      "104/104 [==============================] - 5s 45ms/step - loss: 1.6159 - acc: 0.2885 - val_loss: 1.5932 - val_acc: 0.2593\n",
      "Epoch 2/80\n",
      "104/104 [==============================] - 0s 887us/step - loss: 1.5662 - acc: 0.3269 - val_loss: 1.5937 - val_acc: 0.2593\n",
      "Epoch 3/80\n",
      "104/104 [==============================] - 0s 964us/step - loss: 1.5433 - acc: 0.2981 - val_loss: 1.6039 - val_acc: 0.2222\n",
      "Epoch 4/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 1.4874 - acc: 0.4038 - val_loss: 1.6148 - val_acc: 0.2593\n",
      "Epoch 5/80\n",
      "104/104 [==============================] - 0s 983us/step - loss: 1.4835 - acc: 0.3173 - val_loss: 1.6189 - val_acc: 0.1852\n",
      "Epoch 6/80\n",
      "104/104 [==============================] - 0s 926us/step - loss: 1.4337 - acc: 0.3750 - val_loss: 1.6094 - val_acc: 0.1852\n",
      "Epoch 7/80\n",
      "104/104 [==============================] - 0s 993us/step - loss: 1.4052 - acc: 0.3942 - val_loss: 1.5862 - val_acc: 0.1852\n",
      "Epoch 8/80\n",
      "104/104 [==============================] - 0s 988us/step - loss: 1.3307 - acc: 0.4904 - val_loss: 1.5501 - val_acc: 0.2222\n",
      "Epoch 9/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 1.3225 - acc: 0.4712 - val_loss: 1.4999 - val_acc: 0.2222\n",
      "Epoch 10/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 1.2611 - acc: 0.4712 - val_loss: 1.4417 - val_acc: 0.1852\n",
      "Epoch 11/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 1.2028 - acc: 0.5288 - val_loss: 1.3770 - val_acc: 0.2963\n",
      "Epoch 12/80\n",
      "104/104 [==============================] - 0s 964us/step - loss: 1.1100 - acc: 0.6058 - val_loss: 1.2922 - val_acc: 0.2963\n",
      "Epoch 13/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 1.0813 - acc: 0.6346 - val_loss: 1.2186 - val_acc: 0.4815\n",
      "Epoch 14/80\n",
      "104/104 [==============================] - 0s 916us/step - loss: 0.9621 - acc: 0.6827 - val_loss: 1.2120 - val_acc: 0.4815\n",
      "Epoch 15/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.8501 - acc: 0.7212 - val_loss: 1.2263 - val_acc: 0.4444\n",
      "Epoch 16/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.8362 - acc: 0.6827 - val_loss: 1.0877 - val_acc: 0.4815\n",
      "Epoch 17/80\n",
      "104/104 [==============================] - 0s 993us/step - loss: 0.7616 - acc: 0.7788 - val_loss: 1.0765 - val_acc: 0.5185\n",
      "Epoch 18/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6855 - acc: 0.7885 - val_loss: 1.1143 - val_acc: 0.5556\n",
      "Epoch 19/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6191 - acc: 0.8077 - val_loss: 0.9450 - val_acc: 0.6296\n",
      "Epoch 20/80\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.5715 - acc: 0.8077 - val_loss: 0.9101 - val_acc: 0.6296\n",
      "Epoch 21/80\n",
      "104/104 [==============================] - 0s 935us/step - loss: 0.5725 - acc: 0.8365 - val_loss: 0.8357 - val_acc: 0.6667\n",
      "Epoch 22/80\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.4742 - acc: 0.8462 - val_loss: 0.7279 - val_acc: 0.7407\n",
      "Epoch 23/80\n",
      "104/104 [==============================] - 0s 897us/step - loss: 0.4039 - acc: 0.9038 - val_loss: 0.7378 - val_acc: 0.7407\n",
      "Epoch 24/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3621 - acc: 0.8654 - val_loss: 0.8428 - val_acc: 0.6296\n",
      "Epoch 25/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4908 - acc: 0.8365 - val_loss: 0.7179 - val_acc: 0.8519\n",
      "Epoch 26/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4185 - acc: 0.8654 - val_loss: 0.9221 - val_acc: 0.6667\n",
      "Epoch 27/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3870 - acc: 0.9135 - val_loss: 0.8676 - val_acc: 0.7037\n",
      "Epoch 28/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2965 - acc: 0.9135 - val_loss: 0.8694 - val_acc: 0.6667\n",
      "Epoch 29/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3008 - acc: 0.9038 - val_loss: 1.0045 - val_acc: 0.6667\n",
      "Epoch 30/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2561 - acc: 0.9135 - val_loss: 0.8221 - val_acc: 0.7407\n",
      "Epoch 31/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2635 - acc: 0.9231 - val_loss: 0.8131 - val_acc: 0.7037\n",
      "Epoch 32/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2273 - acc: 0.9519 - val_loss: 0.7175 - val_acc: 0.8148\n",
      "Epoch 33/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1769 - acc: 0.9712 - val_loss: 0.9295 - val_acc: 0.6667\n",
      "Epoch 34/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2504 - acc: 0.9327 - val_loss: 0.8066 - val_acc: 0.7037\n",
      "Epoch 35/80\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.1793 - acc: 0.9519 - val_loss: 0.9245 - val_acc: 0.7037\n",
      "Epoch 36/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1907 - acc: 0.9423 - val_loss: 0.9964 - val_acc: 0.7037\n",
      "Epoch 37/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1531 - acc: 0.9615 - val_loss: 1.0138 - val_acc: 0.6667\n",
      "Epoch 38/80\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.0807 - acc: 0.9904 - val_loss: 1.0096 - val_acc: 0.6667\n",
      "Epoch 39/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0979 - acc: 0.9712 - val_loss: 1.0996 - val_acc: 0.7407\n",
      "Epoch 40/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1054 - acc: 0.9712 - val_loss: 1.0171 - val_acc: 0.7037\n",
      "Epoch 41/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0791 - acc: 0.9808 - val_loss: 1.1476 - val_acc: 0.7037\n",
      "Epoch 42/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1067 - acc: 0.9615 - val_loss: 1.1036 - val_acc: 0.7037\n",
      "Epoch 43/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0531 - acc: 0.9904 - val_loss: 1.2499 - val_acc: 0.7407\n",
      "Epoch 44/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1270 - acc: 0.9712 - val_loss: 1.0404 - val_acc: 0.7037\n",
      "Epoch 45/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1223 - acc: 0.9615 - val_loss: 0.9608 - val_acc: 0.7778\n",
      "Epoch 46/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0411 - acc: 1.0000 - val_loss: 1.3359 - val_acc: 0.7037\n",
      "Epoch 47/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1235 - acc: 0.9615 - val_loss: 1.2890 - val_acc: 0.6667\n",
      "Epoch 48/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0822 - acc: 0.9808 - val_loss: 1.3196 - val_acc: 0.6296\n",
      "Epoch 49/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1121 - acc: 0.9615 - val_loss: 0.9353 - val_acc: 0.7037\n",
      "Epoch 50/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0452 - acc: 1.0000 - val_loss: 0.9365 - val_acc: 0.7778\n",
      "Epoch 51/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0874 - acc: 0.9712 - val_loss: 1.0400 - val_acc: 0.7407\n",
      "Epoch 52/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0386 - acc: 0.9904 - val_loss: 1.3840 - val_acc: 0.7037\n",
      "Epoch 53/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0471 - acc: 0.9904 - val_loss: 1.5729 - val_acc: 0.6667\n",
      "Epoch 54/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0324 - acc: 1.0000 - val_loss: 1.3901 - val_acc: 0.6296\n",
      "Epoch 55/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 1.2931 - val_acc: 0.6667\n",
      "Epoch 56/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0254 - acc: 1.0000 - val_loss: 1.2766 - val_acc: 0.6667\n",
      "Epoch 57/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0302 - acc: 1.0000 - val_loss: 1.3054 - val_acc: 0.7037\n",
      "Epoch 58/80\n",
      "104/104 [==============================] - 0s 974us/step - loss: 0.0273 - acc: 1.0000 - val_loss: 1.3124 - val_acc: 0.7037\n",
      "Epoch 59/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 1.3965 - val_acc: 0.7037\n",
      "Epoch 60/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0166 - acc: 1.0000 - val_loss: 1.5116 - val_acc: 0.6667\n",
      "Epoch 61/80\n",
      "104/104 [==============================] - 0s 950us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 1.6078 - val_acc: 0.6296\n",
      "Epoch 62/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 873us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 1.6762 - val_acc: 0.6296\n",
      "Epoch 63/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0218 - acc: 1.0000 - val_loss: 1.6875 - val_acc: 0.6296\n",
      "Epoch 64/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0241 - acc: 1.0000 - val_loss: 1.7074 - val_acc: 0.7037\n",
      "Epoch 65/80\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 1.6872 - val_acc: 0.7407\n",
      "Epoch 66/80\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 1.6950 - val_acc: 0.7407\n",
      "Epoch 67/80\n",
      "104/104 [==============================] - 0s 877us/step - loss: 0.0275 - acc: 0.9904 - val_loss: 1.6455 - val_acc: 0.7407\n",
      "Epoch 68/80\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 1.6472 - val_acc: 0.7037\n",
      "Epoch 69/80\n",
      "104/104 [==============================] - 0s 935us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 1.6756 - val_acc: 0.7037\n",
      "Epoch 70/80\n",
      "104/104 [==============================] - 0s 848us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 1.7320 - val_acc: 0.7037\n",
      "Epoch 71/80\n",
      "104/104 [==============================] - 0s 897us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 1.7888 - val_acc: 0.6296\n",
      "Epoch 72/80\n",
      "104/104 [==============================] - 0s 993us/step - loss: 0.0229 - acc: 0.9904 - val_loss: 1.6969 - val_acc: 0.7407\n",
      "Epoch 73/80\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 1.6997 - val_acc: 0.7407\n",
      "Epoch 74/80\n",
      "104/104 [==============================] - 0s 971us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 1.7912 - val_acc: 0.7037\n",
      "Epoch 75/80\n",
      "104/104 [==============================] - 0s 955us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 1.8535 - val_acc: 0.7037\n",
      "Epoch 76/80\n",
      "104/104 [==============================] - 0s 935us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 1.8693 - val_acc: 0.7037\n",
      "Epoch 77/80\n",
      "104/104 [==============================] - 0s 877us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 1.7655 - val_acc: 0.7037\n",
      "Epoch 78/80\n",
      "104/104 [==============================] - 0s 905us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 1.6647 - val_acc: 0.6667\n",
      "Epoch 79/80\n",
      "104/104 [==============================] - 0s 970us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 1.9715 - val_acc: 0.6296\n",
      "Epoch 80/80\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 2.0202 - val_acc: 0.6296\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(embedded_x_train,y_train,epochs=80,batch_size=64,shuffle=True,validation_split=0.2,callbacks=[Checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 474us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.236179076541554, 0.6727272738109935]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embedded_x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test accuracy of 67 percent is achieved for a 5 way classification. It is not bad considering. The accuracy could have been much higher if the dataset was larger. Even on a very small dataset our model produced a test accuracy of 67 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some test data and predicted emojis by our model\n",
    "predict=model.predict_classes(embedded_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he did not answer üòì\n",
      "he got a raise üòÅ\n",
      "she got me a present üíì\n",
      "ha ha ha it was so funny üòÅ\n",
      "he is a good friend üòÅ\n",
      "I am upset üòì\n",
      "We had such a lovely dinner tonight üòÅ\n",
      "where is the food üç¥\n",
      "Stop making this joke ha ha ha üòÅ\n",
      "where is the ball ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\" \".join(x_test[i]),emoji.emojize(emoji_dictionary[str(predict[i])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
