{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40340416,  0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playing using random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game over 1/20: high score:-0.4087496643632926\n",
      "game over 2/20: high score:-0.3300978613732684\n",
      "game over 3/20: high score:-0.46269613731745507\n",
      "game over 4/20: high score:-0.4843656601112598\n",
      "game over 5/20: high score:-0.3977204352197913\n",
      "game over 6/20: high score:-0.3901900601228825\n",
      "game over 7/20: high score:-0.47584323408271884\n",
      "game over 8/20: high score:-0.3717156629297182\n",
      "game over 9/20: high score:-0.4084105849086924\n",
      "game over 10/20: high score:-0.3952660301221075\n",
      "game over 11/20: high score:-0.3221162835422451\n",
      "game over 12/20: high score:-0.431525420616796\n",
      "game over 13/20: high score:-0.45194791990148303\n",
      "game over 14/20: high score:-0.42148519433993115\n",
      "game over 15/20: high score:-0.43651691236075424\n",
      "game over 16/20: high score:-0.46806935626579016\n",
      "game over 17/20: high score:-0.48033068977230436\n",
      "game over 18/20: high score:-0.35310134483583744\n",
      "game over 19/20: high score:-0.3271648077956955\n",
      "game over 20/20: high score:-0.42562464107473424\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for e in range(20):\n",
    "    observations=env.reset()\n",
    "    max_pos=observations[0]\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        observations,reward,done,info=env.step(env.action_space.sample())\n",
    "        max_pos=max(max_pos,observations[0])\n",
    "        #print(reward)\n",
    "        if done:\n",
    "            break\n",
    "    print(\"game over {}/{}: high score:{}\".format(e+1,20,max_pos))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_decay=0.90\n",
    "        self.epsilon_min=0.01\n",
    "        self.model=self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(32,input_shape=(2,),activation='relu'))\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(Dense(3,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer='Adam')\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def act(self,state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values=self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def train(self,batch_size=32):\n",
    "        minibatch=random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            if not done:\n",
    "                target=reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f=self.model.predict(state)\n",
    "            target_f[0][action]=target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "            if self.epsilon>self.epsilon_min:\n",
    "                self.epsilon*=self.epsilon_decay\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=100\n",
    "output_dir=\"mountain_car/\"\n",
    "state_size=2\n",
    "action_size=3\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(state_size,action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent on generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that we have tweaked the reward. We have made the reward such that the agent tries to drive forward when velocity is positive and drives backwards when the velocity is negative.\n",
    "* The reward returned by gym is always -1 if the car is not able to reach the flag which is situated at the position +0.5. Thus it would have taken a lot of exploration if we were to use that reward to frame our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1/100: Unsuccessful attempt, high score: -0.2463605590327244\n",
      "Game 2/100: Unsuccessful attempt, high score: -0.460094356573934\n",
      "Game 3/100: Unsuccessful attempt, high score: -0.46296546958087514\n",
      "Game 4/100: Unsuccessful attempt, high score: -0.4918548234129807\n",
      "Game 5/100: Unsuccessful attempt, high score: -0.5731088618689911\n",
      "Game 6/100: Unsuccessful attempt, high score: -0.3588716876777703\n",
      "Game 7/100: Unsuccessful attempt, high score: -0.48005745065901567\n",
      "Game 8/100: Unsuccessful attempt, high score: -0.4065040319531399\n",
      "Game 9/100: Unsuccessful attempt, high score: -0.4450758928501298\n",
      "Game 10/100: Unsuccessful attempt, high score: 0.14496182604197166\n",
      "Game 11/100: Unsuccessful attempt, high score: -0.484008043394069\n",
      "Game 12/100: Unsuccessful attempt, high score: -0.40446890258771373\n",
      "Game 13/100: Unsuccessful attempt, high score: -0.5074997430873608\n",
      "Game 14/100: Unsuccessful attempt, high score: -0.3457247413046742\n",
      "Game 15/100: Unsuccessful attempt, high score: -0.48137241762271077\n",
      "Game 16/100: Unsuccessful attempt, high score: -0.4879207033575242\n",
      "Game 17/100: Unsuccessful attempt, high score: -0.45273233437884797\n",
      "Game 18/100: Unsuccessful attempt, high score: -0.2276351057756456\n",
      "Game 19/100: Unsuccessful attempt, high score: -0.4713919331031475\n",
      "Game 20/100: Unsuccessful attempt, high score: -0.4285023919994393\n",
      "Game 21/100: Unsuccessful attempt, high score: -0.3501468760629591\n",
      "Game 22/100: Completed Successfully\n",
      "Game 23/100: Unsuccessful attempt, high score: -0.4878415689628668\n",
      "Game 24/100: Unsuccessful attempt, high score: -0.550514579722751\n",
      "Game 25/100: Completed Successfully\n",
      "Game 26/100: Unsuccessful attempt, high score: -0.5361237539333389\n",
      "Game 27/100: Unsuccessful attempt, high score: -0.4242974045724166\n",
      "Game 28/100: Unsuccessful attempt, high score: -0.19578728797181813\n",
      "Game 29/100: Unsuccessful attempt, high score: -0.4297421338834855\n",
      "Game 30/100: Completed Successfully\n",
      "Game 31/100: Unsuccessful attempt, high score: -0.24615738327515718\n",
      "Game 32/100: Unsuccessful attempt, high score: -0.19890479398157807\n",
      "Game 33/100: Unsuccessful attempt, high score: -0.532896010549283\n",
      "Game 34/100: Unsuccessful attempt, high score: -0.17976287227543242\n",
      "Game 35/100: Unsuccessful attempt, high score: -0.5018649328431439\n",
      "Game 36/100: Unsuccessful attempt, high score: -0.4597348258981442\n",
      "Game 37/100: Completed Successfully\n",
      "Game 38/100: Completed Successfully\n",
      "Game 39/100: Completed Successfully\n",
      "Game 40/100: Completed Successfully\n",
      "Game 41/100: Unsuccessful attempt, high score: -0.49500003012661575\n",
      "Game 42/100: Completed Successfully\n",
      "Game 43/100: Unsuccessful attempt, high score: -0.34340098638986627\n",
      "Game 44/100: Completed Successfully\n",
      "Game 45/100: Completed Successfully\n",
      "Game 46/100: Unsuccessful attempt, high score: 0.09576738453859797\n",
      "Game 47/100: Completed Successfully\n",
      "Game 48/100: Completed Successfully\n",
      "Game 49/100: Completed Successfully\n",
      "Game 50/100: Unsuccessful attempt, high score: -0.2160921471668376\n",
      "Game 51/100: Completed Successfully\n",
      "Game 52/100: Completed Successfully\n",
      "Game 53/100: Completed Successfully\n",
      "Game 54/100: Completed Successfully\n",
      "Game 55/100: Completed Successfully\n",
      "Game 56/100: Unsuccessful attempt, high score: -0.4846607556151358\n",
      "Game 57/100: Completed Successfully\n",
      "Game 58/100: Unsuccessful attempt, high score: -0.014291398295181489\n",
      "Game 59/100: Unsuccessful attempt, high score: 0.3777936007786232\n",
      "Game 60/100: Unsuccessful attempt, high score: -0.34482679269743527\n",
      "Game 61/100: Completed Successfully\n",
      "Game 62/100: Completed Successfully\n",
      "Game 63/100: Unsuccessful attempt, high score: -0.47496746291266445\n",
      "Game 64/100: Unsuccessful attempt, high score: -0.5579156379554133\n",
      "Game 65/100: Unsuccessful attempt, high score: -0.5009559502939622\n",
      "Game 66/100: Completed Successfully\n",
      "Game 67/100: Completed Successfully\n",
      "Game 68/100: Completed Successfully\n",
      "Game 69/100: Completed Successfully\n",
      "Game 70/100: Completed Successfully\n",
      "Game 71/100: Completed Successfully\n",
      "Game 72/100: Completed Successfully\n",
      "Game 73/100: Unsuccessful attempt, high score: -0.5662864449431899\n",
      "Game 74/100: Unsuccessful attempt, high score: -0.5723490907546881\n",
      "Game 75/100: Unsuccessful attempt, high score: -0.4890735273818607\n",
      "Game 76/100: Completed Successfully\n",
      "Game 77/100: Completed Successfully\n",
      "Game 78/100: Completed Successfully\n",
      "Game 79/100: Completed Successfully\n",
      "Game 80/100: Completed Successfully\n",
      "Game 81/100: Completed Successfully\n",
      "Game 82/100: Completed Successfully\n",
      "Game 83/100: Unsuccessful attempt, high score: -0.5961053053967855\n",
      "Game 84/100: Unsuccessful attempt, high score: -0.48810534321312016\n",
      "Game 85/100: Completed Successfully\n",
      "Game 86/100: Completed Successfully\n",
      "Game 87/100: Completed Successfully\n",
      "Game 88/100: Completed Successfully\n",
      "Game 89/100: Unsuccessful attempt, high score: -0.41608578118878126\n",
      "Game 90/100: Completed Successfully\n",
      "Game 91/100: Unsuccessful attempt, high score: -0.5729801153859505\n",
      "Game 92/100: Completed Successfully\n",
      "Game 93/100: Completed Successfully\n",
      "Game 94/100: Completed Successfully\n",
      "Game 95/100: Completed Successfully\n",
      "Game 96/100: Completed Successfully\n",
      "Game 97/100: Completed Successfully\n",
      "Game 98/100: Completed Successfully\n",
      "Game 99/100: Unsuccessful attempt, high score: -0.45681238682466035\n",
      "Game 100/100: Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,[1,state_size])\n",
    "    flag=False\n",
    "    max_pos=(state[0][0])\n",
    "    flag=False\n",
    "    for time in range(200):\n",
    "        env.render()\n",
    "        action=agent.act(state)\n",
    "        next_state,reward,done,info=env.step(action)\n",
    "        next_state=np.reshape(next_state,[1,state_size])\n",
    "        max_pos=max(max_pos,(next_state[0][0]))\n",
    "        reward=(action-1)*state[0][1]\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        if state[0][0]>=0.5:\n",
    "            flag=True\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "    if flag:\n",
    "        print(\"Game {}/{}: Completed Successfully\".format(e+1,n_episodes))\n",
    "    else:\n",
    "        print(\"Game {}/{}: Unsuccessful attempt, high score: {}\".format(e+1,n_episodes,max_pos))\n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "    if e%10==0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent is able to complete the game more often as the training progresses. We have successully implemented an agent which plays the mountain car game for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
